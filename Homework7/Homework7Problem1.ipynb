{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90uyzkH6Gfl6"
   },
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2fT7t4ZdGY9A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waAPJLfLGsM7"
   },
   "source": [
    "# Problem 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6sEZvlYyE7w3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/300, Loss: 2.243717732941708, Training Time: 11.24 seconds\n",
      "Epoch 11/300, Loss: 1.501794728476678, Training Time: 15.30 seconds\n",
      "Epoch 21/300, Loss: 1.314770680864144, Training Time: 16.81 seconds\n",
      "Epoch 31/300, Loss: 1.1802404245452198, Training Time: 15.08 seconds\n",
      "Epoch 41/300, Loss: 1.0679627315467581, Training Time: 15.36 seconds\n",
      "Epoch 51/300, Loss: 0.9713299901741544, Training Time: 15.50 seconds\n",
      "Epoch 61/300, Loss: 0.884684456888672, Training Time: 15.06 seconds\n",
      "Epoch 71/300, Loss: 0.8006320481791216, Training Time: 15.60 seconds\n",
      "Epoch 81/300, Loss: 0.7196936319246316, Training Time: 16.01 seconds\n",
      "Epoch 91/300, Loss: 0.6396148684994339, Training Time: 17.26 seconds\n",
      "Epoch 101/300, Loss: 0.5578595217688919, Training Time: 15.91 seconds\n",
      "Epoch 111/300, Loss: 0.4786670754099136, Training Time: 15.82 seconds\n",
      "Epoch 121/300, Loss: 0.4006964515732682, Training Time: 14.23 seconds\n",
      "Epoch 131/300, Loss: 0.3298580515986818, Training Time: 15.09 seconds\n",
      "Epoch 141/300, Loss: 0.2651865439837241, Training Time: 15.74 seconds\n",
      "Epoch 151/300, Loss: 0.21053429324265635, Training Time: 15.26 seconds\n",
      "Epoch 161/300, Loss: 0.16564190947948515, Training Time: 15.26 seconds\n",
      "Epoch 171/300, Loss: 0.1307589350854192, Training Time: 15.71 seconds\n",
      "Epoch 181/300, Loss: 0.10337974869972452, Training Time: 15.71 seconds\n",
      "Epoch 191/300, Loss: 0.08305655847615598, Training Time: 15.24 seconds\n",
      "Epoch 201/300, Loss: 0.06769983350868573, Training Time: 16.09 seconds\n",
      "Epoch 211/300, Loss: 0.055898425648050844, Training Time: 16.11 seconds\n",
      "Epoch 221/300, Loss: 0.04711108893880149, Training Time: 15.20 seconds\n",
      "Epoch 231/300, Loss: 0.0404328188818434, Training Time: 15.25 seconds\n",
      "Epoch 241/300, Loss: 0.035045878542468066, Training Time: 15.34 seconds\n",
      "Epoch 251/300, Loss: 0.030834139915435668, Training Time: 16.10 seconds\n",
      "Epoch 261/300, Loss: 0.027373039728159184, Training Time: 15.77 seconds\n",
      "Epoch 271/300, Loss: 0.024508369301714936, Training Time: 15.37 seconds\n",
      "Epoch 281/300, Loss: 0.02213036474150122, Training Time: 15.45 seconds\n",
      "Epoch 291/300, Loss: 0.020175681252966222, Training Time: 15.14 seconds\n",
      "Total Training Time: 4656.42 seconds\n",
      "Test Accuracy: 69.13%\n",
      "Final F1 Score: 0.6899\n",
      "Final Confusion Matrix:\n",
      "[[744  17  47  19  20   7  15  10  82  39]\n",
      " [ 25 793  12   8   3   6  13   6  34 100]\n",
      " [ 65  12 542  63  91  77  87  37  10  16]\n",
      " [ 26  12  83 507  60 168  68  33  19  24]\n",
      " [ 23   4  78  70 607  56  66  78  14   4]\n",
      " [ 14   4  39 165  42 628  29  55  18   6]\n",
      " [  5  10  38  55  35  37 800   8   9   3]\n",
      " [ 15   7  25  48  58  78  11 736   6  16]\n",
      " [ 57  49  11  20   8   5   4   6 812  28]\n",
      " [ 35  97   8  16   3  17   9  26  45 744]]\n",
      "Final Test Accuracy: 69.13%\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()\n",
    "\n",
    "# Calculate and print F1 score at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# After training is complete, print final evaluation accuracy\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idprKeewG8ma"
   },
   "source": [
    "# Problem 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WC0ESwvtG8ED"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/300, Loss: 2.280784971878657, Training Time: 11.44 seconds\n",
      "Epoch 11/300, Loss: 1.5271730456510773, Training Time: 13.70 seconds\n",
      "Epoch 21/300, Loss: 1.292213365016386, Training Time: 13.82 seconds\n",
      "Epoch 31/300, Loss: 1.147958550687946, Training Time: 13.74 seconds\n",
      "Epoch 41/300, Loss: 1.0108811901048627, Training Time: 13.74 seconds\n",
      "Epoch 51/300, Loss: 0.8800822478121199, Training Time: 13.83 seconds\n",
      "Epoch 61/300, Loss: 0.7520235096630843, Training Time: 14.10 seconds\n",
      "Epoch 71/300, Loss: 0.6235002449253941, Training Time: 13.93 seconds\n",
      "Epoch 81/300, Loss: 0.49445229639177735, Training Time: 14.15 seconds\n",
      "Epoch 91/300, Loss: 0.36919106499237175, Training Time: 14.40 seconds\n",
      "Epoch 101/300, Loss: 0.26064762671280395, Training Time: 13.60 seconds\n",
      "Epoch 111/300, Loss: 0.17583338660962136, Training Time: 13.70 seconds\n",
      "Epoch 121/300, Loss: 0.11732674452483349, Training Time: 13.70 seconds\n",
      "Epoch 131/300, Loss: 0.08077849173332419, Training Time: 13.84 seconds\n",
      "Epoch 141/300, Loss: 0.0581985547652711, Training Time: 13.82 seconds\n",
      "Epoch 151/300, Loss: 0.043857908000231094, Training Time: 13.63 seconds\n",
      "Epoch 161/300, Loss: 0.03459124151817368, Training Time: 13.91 seconds\n",
      "Epoch 171/300, Loss: 0.0282249807754098, Training Time: 13.80 seconds\n",
      "Epoch 181/300, Loss: 0.02357481182088404, Training Time: 13.82 seconds\n",
      "Epoch 191/300, Loss: 0.020144849342753745, Training Time: 13.74 seconds\n",
      "Epoch 201/300, Loss: 0.017532262472373904, Training Time: 14.07 seconds\n",
      "Epoch 211/300, Loss: 0.015461398170703588, Training Time: 13.68 seconds\n",
      "Epoch 221/300, Loss: 0.013784357850366007, Training Time: 13.78 seconds\n",
      "Epoch 231/300, Loss: 0.012410571125795697, Training Time: 13.82 seconds\n",
      "Epoch 241/300, Loss: 0.011277948575251547, Training Time: 13.66 seconds\n",
      "Epoch 251/300, Loss: 0.010316362019742618, Training Time: 13.83 seconds\n",
      "Epoch 261/300, Loss: 0.009512303953828372, Training Time: 13.77 seconds\n",
      "Epoch 271/300, Loss: 0.008787184660715977, Training Time: 13.80 seconds\n",
      "Epoch 281/300, Loss: 0.00817784098158007, Training Time: 13.73 seconds\n",
      "Epoch 291/300, Loss: 0.0076383733139385275, Training Time: 13.76 seconds\n",
      "Total Training Time: 4147.21 seconds\n",
      "Test Accuracy: 67.99%\n",
      "Final F1 Score: 0.6779\n",
      "Final Confusion Matrix:\n",
      "[[770  21  43  19  15   8  10   7  70  37]\n",
      " [ 27 799  11  10   2   7  10   8  26 100]\n",
      " [ 78   7 521  69 104  70  67  48  17  19]\n",
      " [ 32  10  65 466  63 191  88  42  18  25]\n",
      " [ 37   4  74  72 604  42  63  81  14   9]\n",
      " [ 23   7  47 158  50 607  30  57  14   7]\n",
      " [ 12  12  44  58  42  33 775   8   6  10]\n",
      " [ 22   5  28  40  60  76   7 738   5  19]\n",
      " [ 76  54  10  16  11   9   8   6 780  30]\n",
      " [ 46 109   8  18   5  18   9  20  28 739]]\n",
      "Final Test Accuracy: 67.99%\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the extended CNN model\n",
    "class ExtendedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExtendedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool1(self.relu2(self.conv2(x)))\n",
    "        x = self.pool2(self.relu3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the extended model\n",
    "model = ExtendedCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()\n",
    "\n",
    "# Calculate and print F1 score at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# After training is complete, print final evaluation accuracy\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
