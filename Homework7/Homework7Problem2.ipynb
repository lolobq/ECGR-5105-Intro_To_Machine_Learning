{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxuNLXSZwEoR"
   },
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7udR_CObSrQQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvzD3Wf9wI2a"
   },
   "source": [
    "# Problem 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZiAlLbR5v_1r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/300, Loss: 1.833100466014784, Training Time: 54.60 seconds\n",
      "Epoch 11/300, Loss: 0.6835234948360097, Training Time: 44.71 seconds\n",
      "Epoch 21/300, Loss: 0.18683619255109515, Training Time: 44.57 seconds\n",
      "Epoch 31/300, Loss: 0.030607760501572923, Training Time: 44.43 seconds\n",
      "Epoch 41/300, Loss: 0.013625685093731469, Training Time: 44.35 seconds\n",
      "Epoch 51/300, Loss: 0.008510028143547942, Training Time: 44.29 seconds\n",
      "Epoch 61/300, Loss: 0.004117418974822582, Training Time: 44.45 seconds\n",
      "Epoch 71/300, Loss: 0.0031953321566181186, Training Time: 44.62 seconds\n",
      "Epoch 81/300, Loss: 0.007624684968237381, Training Time: 44.45 seconds\n",
      "Epoch 91/300, Loss: 0.011092630650057955, Training Time: 44.68 seconds\n",
      "Epoch 101/300, Loss: 0.0029602833423817703, Training Time: 44.20 seconds\n",
      "Epoch 111/300, Loss: 0.003523125251739636, Training Time: 44.40 seconds\n",
      "Epoch 121/300, Loss: 0.0015123425450500833, Training Time: 44.47 seconds\n",
      "Epoch 131/300, Loss: 0.004666312349006853, Training Time: 44.47 seconds\n",
      "Epoch 141/300, Loss: 0.0034633997357045505, Training Time: 44.55 seconds\n",
      "Epoch 151/300, Loss: 0.0013456726832503792, Training Time: 44.48 seconds\n",
      "Epoch 161/300, Loss: 0.0012017858467641516, Training Time: 44.30 seconds\n",
      "Epoch 171/300, Loss: 0.0012421265009811054, Training Time: 44.23 seconds\n",
      "Epoch 181/300, Loss: 0.0009125048444174124, Training Time: 44.31 seconds\n",
      "Epoch 191/300, Loss: 0.0015406203631329817, Training Time: 44.53 seconds\n",
      "Epoch 201/300, Loss: 0.0014702445949813204, Training Time: 44.54 seconds\n",
      "Epoch 211/300, Loss: 0.0011744650025793072, Training Time: 44.27 seconds\n",
      "Epoch 221/300, Loss: 0.0020103676543302943, Training Time: 44.47 seconds\n",
      "Epoch 231/300, Loss: 0.0012342113642515662, Training Time: 44.64 seconds\n",
      "Epoch 241/300, Loss: 0.0007143789324756114, Training Time: 44.32 seconds\n",
      "Epoch 251/300, Loss: 0.0011246528299341527, Training Time: 44.48 seconds\n",
      "Epoch 261/300, Loss: 0.0009416630491056084, Training Time: 44.47 seconds\n",
      "Epoch 271/300, Loss: 0.0015242519031018716, Training Time: 44.45 seconds\n",
      "Epoch 281/300, Loss: 0.0006857533808355085, Training Time: 44.29 seconds\n",
      "Epoch 291/300, Loss: 0.0011161646848205653, Training Time: 44.48 seconds\n",
      "Total Training Time: 13354.50 seconds\n",
      "Final Test Accuracy: 74.06%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Calculate and print F1 score, precision, and recall at the end\u001b[39;00m\n\u001b[0;32m    144\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels, all_predicted, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 145\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(all_labels, all_predicted, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    146\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(all_labels, all_predicted, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define ResNet-10 model\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.in_channels = n_chans1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the ResNet-10 model with ResNet blocks\n",
    "model = ResNet10(ResidualBlock, [4, 3, 3]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate and print F1 score, precision, and recall at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "print(f'Final Precision: {precision:.4f}')\n",
    "print(f'Final Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final F1 Score: 0.7401\n",
      "Final Precision: 0.7404\n",
      "Final Recall: 0.7406\n",
      "Final Confusion Matrix:\n",
      "[[784  26  47  16  15  12   8  10  48  34]\n",
      " [ 19 881   7   4   4   3   4   4   9  65]\n",
      " [ 69   4 617  56  86  56  66  23  14   9]\n",
      " [ 24   9  62 563  56 163  68  23   8  24]\n",
      " [ 25   3  77  61 672  34  57  64   7   0]\n",
      " [ 15   5  43 168  43 658   9  43   6  10]\n",
      " [ 11   5  45  58  43  30 788  10   7   3]\n",
      " [ 16   3  26  36  70  73   8 749   4  15]\n",
      " [ 79  31   6  13  10   1   9   5 827  19]\n",
      " [ 23  59   4   4   7   6   5   5  20 867]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "print(f'Final Precision: {precision:.4f}')\n",
    "print(f'Final Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUi7kFvCxGlw"
   },
   "source": [
    "# Problem 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHZIZZsVzn37"
   },
   "source": [
    "Weight Decay with Lambda of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T9_UVEARxJIT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/300, Loss: 1.8251291940279324, Training Time: 49.22 seconds\n",
      "Epoch 11/300, Loss: 0.6921585613428174, Training Time: 48.03 seconds\n",
      "Epoch 21/300, Loss: 0.205784567043452, Training Time: 49.06 seconds\n",
      "Epoch 31/300, Loss: 0.03428162000192058, Training Time: 48.17 seconds\n",
      "Epoch 41/300, Loss: 0.013138540848658498, Training Time: 48.52 seconds\n",
      "Epoch 51/300, Loss: 0.009550689951852535, Training Time: 39.53 seconds\n",
      "Epoch 61/300, Loss: 0.005800260849224994, Training Time: 39.48 seconds\n",
      "Epoch 71/300, Loss: 0.0040980548211706855, Training Time: 39.54 seconds\n",
      "Epoch 81/300, Loss: 0.0038474751733894915, Training Time: 39.46 seconds\n",
      "Epoch 91/300, Loss: 0.006307307020341263, Training Time: 39.42 seconds\n",
      "Epoch 101/300, Loss: 0.0027406423845711876, Training Time: 39.39 seconds\n",
      "Epoch 111/300, Loss: 0.008143035434064387, Training Time: 39.40 seconds\n",
      "Epoch 121/300, Loss: 0.003394888263032593, Training Time: 39.42 seconds\n",
      "Epoch 131/300, Loss: 0.0015190996756941518, Training Time: 39.51 seconds\n",
      "Epoch 141/300, Loss: 0.0030725654136836695, Training Time: 39.44 seconds\n",
      "Epoch 151/300, Loss: 0.0016957019669208628, Training Time: 39.71 seconds\n",
      "Epoch 161/300, Loss: 0.0026929694335952954, Training Time: 39.40 seconds\n",
      "Epoch 171/300, Loss: 0.0019397667836254376, Training Time: 39.42 seconds\n",
      "Epoch 181/300, Loss: 0.0017789728576330828, Training Time: 39.43 seconds\n",
      "Epoch 191/300, Loss: 0.0018614220655570283, Training Time: 39.47 seconds\n",
      "Epoch 201/300, Loss: 0.002259915235743839, Training Time: 39.41 seconds\n",
      "Epoch 211/300, Loss: 0.0015546896120817984, Training Time: 39.45 seconds\n",
      "Epoch 221/300, Loss: 0.001717956793737774, Training Time: 39.46 seconds\n",
      "Epoch 231/300, Loss: 0.0015524954532759616, Training Time: 39.50 seconds\n",
      "Epoch 241/300, Loss: 0.011511308184373514, Training Time: 39.45 seconds\n",
      "Epoch 251/300, Loss: 0.0023829478591405415, Training Time: 39.42 seconds\n",
      "Epoch 261/300, Loss: 0.0017655518194309215, Training Time: 39.42 seconds\n",
      "Epoch 271/300, Loss: 0.0023017204768942014, Training Time: 39.67 seconds\n",
      "Epoch 281/300, Loss: 0.0012230518942392464, Training Time: 39.68 seconds\n",
      "Epoch 291/300, Loss: 0.00395170132809109, Training Time: 39.59 seconds\n",
      "Total Training Time: 12253.12 seconds\n",
      "Final Test Accuracy: 71.88%\n",
      "Final F1 Score: 0.7192\n",
      "Final Precision: 0.7322\n",
      "Final Recall: 0.7188\n",
      "Final Confusion Matrix:\n",
      "[[625  20 126  17  29   8  14  17 103  41]\n",
      " [ 11 849   9   6   6   8  15   3  18  75]\n",
      " [ 42   0 726  35  72  35  64  19   7   0]\n",
      " [  5   3 160 440  74 176  95  21  15  11]\n",
      " [  7   0 137  25 699  37  52  36   6   1]\n",
      " [  3   1 100 115  61 648  29  36   2   5]\n",
      " [  4   2  93  30  34  31 802   2   2   0]\n",
      " [  8   1  73  21 112  74   7 690   5   9]\n",
      " [ 37  20  21   9  13   5   9   2 864  20]\n",
      " [ 11  58  17   4   9  12  10  11  23 845]]\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define ResNet-10 model\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.in_channels = n_chans1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the ResNet-10 model with ResNet blocks\n",
    "model = ResNet10(ResidualBlock, [4, 3, 3]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate and print F1 score, precision, and recall at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "print(f'Final Precision: {precision:.4f}')\n",
    "print(f'Final Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DsWD4qg0RQp"
   },
   "source": [
    "Dropout with p = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9Q7ZtmBU0exz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/300, Loss: 2.1646225844197873, Training Time: 41.76 seconds\n",
      "Epoch 11/300, Loss: 1.4325516841295736, Training Time: 41.62 seconds\n",
      "Epoch 21/300, Loss: 1.1517963039753076, Training Time: 41.58 seconds\n",
      "Epoch 31/300, Loss: 0.9842802292245734, Training Time: 41.65 seconds\n",
      "Epoch 41/300, Loss: 0.8802655400217646, Training Time: 41.77 seconds\n",
      "Epoch 51/300, Loss: 0.8018939947075856, Training Time: 41.39 seconds\n",
      "Epoch 61/300, Loss: 0.7375333729150045, Training Time: 41.41 seconds\n",
      "Epoch 71/300, Loss: 0.6801815891585996, Training Time: 41.43 seconds\n",
      "Epoch 81/300, Loss: 0.6323559718287509, Training Time: 41.39 seconds\n",
      "Epoch 91/300, Loss: 0.5870988661675807, Training Time: 41.36 seconds\n",
      "Epoch 101/300, Loss: 0.5508497335645549, Training Time: 41.40 seconds\n",
      "Epoch 111/300, Loss: 0.515632739952763, Training Time: 41.38 seconds\n",
      "Epoch 121/300, Loss: 0.4933827482449734, Training Time: 41.50 seconds\n",
      "Epoch 131/300, Loss: 0.4628690390483193, Training Time: 41.42 seconds\n",
      "Epoch 141/300, Loss: 0.4397674406237919, Training Time: 41.39 seconds\n",
      "Epoch 151/300, Loss: 0.42061272728473637, Training Time: 41.45 seconds\n",
      "Epoch 161/300, Loss: 0.4025026918067347, Training Time: 41.42 seconds\n",
      "Epoch 171/300, Loss: 0.3842091825040405, Training Time: 41.38 seconds\n",
      "Epoch 181/300, Loss: 0.36827285511566854, Training Time: 41.44 seconds\n",
      "Epoch 191/300, Loss: 0.35010058469022326, Training Time: 41.43 seconds\n",
      "Epoch 201/300, Loss: 0.33820655326480453, Training Time: 41.46 seconds\n",
      "Epoch 211/300, Loss: 0.32189716167195376, Training Time: 41.45 seconds\n",
      "Epoch 221/300, Loss: 0.3122823081072182, Training Time: 41.38 seconds\n",
      "Epoch 231/300, Loss: 0.29997012803278617, Training Time: 41.43 seconds\n",
      "Epoch 241/300, Loss: 0.28673251620148454, Training Time: 41.50 seconds\n",
      "Epoch 251/300, Loss: 0.27519663376614567, Training Time: 41.38 seconds\n",
      "Epoch 261/300, Loss: 0.26565253125775196, Training Time: 41.43 seconds\n",
      "Epoch 271/300, Loss: 0.25499688536690934, Training Time: 41.42 seconds\n",
      "Epoch 281/300, Loss: 0.2481081208304676, Training Time: 41.38 seconds\n",
      "Epoch 291/300, Loss: 0.23845631339589654, Training Time: 41.38 seconds\n",
      "Total Training Time: 12440.79 seconds\n",
      "Final Test Accuracy: 83.11%\n",
      "Final F1 Score: 0.8312\n",
      "Final Precision: 0.8455\n",
      "Final Recall: 0.8311\n",
      "Final Confusion Matrix:\n",
      "[[729   3  78  12  38   0  24   3  91  22]\n",
      " [  4 881   3   2   9   3  21   0  30  47]\n",
      " [ 23   0 854  12  34   6  55  10   6   0]\n",
      " [  4   0  93 724  52  44  62  13   7   1]\n",
      " [  1   0  59  15 876   1  41   5   2   0]\n",
      " [  3   0  76 149  65 654  44   8   0   1]\n",
      " [  2   0  22   9   6   1 957   2   1   0]\n",
      " [  3   0  30  15 130  11  14 795   2   0]\n",
      " [  7   3   8   7   7   1  14   0 951   2]\n",
      " [ 13  23   9  13  12   0  14   2  24 890]]\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_prob=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(dropout_prob)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define ResNet-10 model\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64, dropout_prob=0.3):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.in_channels = n_chans1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1, dropout_prob=dropout_prob)\n",
    "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2, dropout_prob=dropout_prob)\n",
    "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2, dropout_prob=dropout_prob)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride, dropout_prob):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, dropout_prob))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the ResNet-10 model with ResNet blocks\n",
    "model = ResNet10(ResidualBlock, [4, 3, 3], dropout_prob=0.3).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate and print F1 score, precision, and recall at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "print(f'Final Precision: {precision:.4f}')\n",
    "print(f'Final Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbLXAJh72ALk"
   },
   "source": [
    "Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOFskX9B2DPF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset to calculate mean and std\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformation with calculated mean and std\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with normalization\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define ResNet-10 model\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, n_chans1=64):\n",
    "        super(ResNet10, self).__init__()\n",
    "        self.in_channels = n_chans1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_chans1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = self._make_layer(block, n_chans1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, n_chans1 * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, n_chans1 * 4, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(n_chans1 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the ResNet-10 model with ResNet blocks and batch normalization\n",
    "model = ResNet10(ResidualBlock, [4, 3, 3]).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "total_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_training_time = total_end_time - total_start_time\n",
    "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate and print F1 score, precision, and recall at the end\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "precision = precision_score(all_labels, all_predicted, average='weighted')\n",
    "recall = recall_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "print(f'Final F1 Score: {f1:.4f}')\n",
    "print(f'Final Precision: {precision:.4f}')\n",
    "print(f'Final Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate and print confusion matrix at the end\n",
    "cm = confusion_matrix(all_labels, all_predicted)\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
