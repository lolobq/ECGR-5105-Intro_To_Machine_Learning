{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA18ORH6/+MWS6D3035XmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lolobq/ECGR-5105-Intro_To_Machine_Learning/blob/main/Homework6/Homework6Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Data"
      ],
      "metadata": {
        "id": "Nb84Ru-jc6bk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8fYeoOqhbqmT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1a"
      ],
      "metadata": {
        "id": "HYh49jPmhvWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop a Fully Connected Neural Network with only one hidden layer (size of 32) to predict the housing value for the housing dataset. Make sure to include all input features. Compare your training loss value and validation results against the linear regression you implemented in Homework 5. Can you compare your model complexity (number of trainable parameters) against linear regression? Note: Perform 20%, and 80% split for training and validation."
      ],
      "metadata": {
        "id": "k9IFfNgVh5SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import csv data from my GitHub repo\n",
        "housing_url = 'https://raw.githubusercontent.com/lolobq/ECGR-5105-Intro_To_Machine_Learning/master/Homework6/Housing.csv'\n",
        "data = pd.read_csv(housing_url)\n",
        "\n",
        "# Map string variables to binary values\n",
        "variable_list = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "def binary_mapping(x):\n",
        "  return x.map({'no' : 0, 'yes' : 1})\n",
        "\n",
        "data[variable_list] = data[variable_list].apply(binary_mapping)\n",
        "data = data.drop('furnishingstatus', axis=1)\n",
        "\n",
        "# Assuming the target variable is 'housing_value', adjust accordingly\n",
        "y = data['price'].values\n",
        "data = data.drop('price', axis=1)\n",
        "x = data.values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "# Standardize the output features\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.fit_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 1)\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5000\n",
        "for epoch in range(epochs+1):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val_tensor)\n",
        "        val_loss = criterion(val_outputs.squeeze(), y_val_tensor)\n",
        "        if epoch % 500 == 0:\n",
        "          print(f'Epoch {epoch}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTBUZMhpiEd1",
        "outputId": "3e84f0bf-1e4c-4020-9351-7ee03d35158a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000, Training Loss: 0.8740482330322266, Validation Loss: 1.2669895887374878\n",
            "Epoch 500/5000, Training Loss: 0.270549476146698, Validation Loss: 0.4624500572681427\n",
            "Epoch 1000/5000, Training Loss: 0.285550057888031, Validation Loss: 0.48047706484794617\n",
            "Epoch 1500/5000, Training Loss: 0.30756011605262756, Validation Loss: 0.47917163372039795\n",
            "Epoch 2000/5000, Training Loss: 0.39476126432418823, Validation Loss: 0.47147655487060547\n",
            "Epoch 2500/5000, Training Loss: 0.24433428049087524, Validation Loss: 0.4641323387622833\n",
            "Epoch 3000/5000, Training Loss: 0.09919007867574692, Validation Loss: 0.4574287533760071\n",
            "Epoch 3500/5000, Training Loss: 0.30647051334381104, Validation Loss: 0.4536994397640228\n",
            "Epoch 4000/5000, Training Loss: 0.15413716435432434, Validation Loss: 0.44924670457839966\n",
            "Epoch 4500/5000, Training Loss: 0.37633734941482544, Validation Loss: 0.44976645708084106\n",
            "Epoch 5000/5000, Training Loss: 0.25254371762275696, Validation Loss: 0.44664818048477173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1b"
      ],
      "metadata": {
        "id": "l8SOgeYyme__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will increase the network complexity by adding two additional hidden layers, the hidden layers overall. My suggestions for the size of layers are: 32, 64, 16, respectively. Please redesign the network and compare your training loss value and validation results against the linear regression you implemented in Homework 5 and Problem 1.a. Can you compare your model complexity? Note: Use the same 20%, and 80% split for training and validation."
      ],
      "metadata": {
        "id": "63DyQN3fmnre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(housing_url)\n",
        "\n",
        "# Map string variables to binary values\n",
        "variable_list = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "def binary_mapping(x):\n",
        "    return x.map({'no': 0, 'yes': 1})\n",
        "\n",
        "data[variable_list] = data[variable_list].apply(binary_mapping)\n",
        "data = data.drop('furnishingstatus', axis=1)\n",
        "\n",
        "# Assuming the target variable is 'housing_value', adjust accordingly\n",
        "y = data['price'].values\n",
        "data = data.drop('price', axis=1)\n",
        "x = data.values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "# Standardize the output features\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.fit_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model with the first hidden layer having 32 neurons\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 32),  # First hidden layer with 32 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 64),  # Second hidden layer with 64 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 16),  # Third hidden layer with 16 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(16, 1)  # Output layer with 1 neuron\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5000\n",
        "for epoch in range(epochs + 1):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val_tensor)\n",
        "        val_loss = criterion(val_outputs.squeeze(), y_val_tensor)\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch {epoch}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnAEJAVFmhIr",
        "outputId": "cc08d137-b892-4fc6-d82a-708e4120a281"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000, Training Loss: 1.186692237854004, Validation Loss: 1.1106796264648438\n",
            "Epoch 500/5000, Training Loss: 0.26614847779273987, Validation Loss: 0.41631677746772766\n",
            "Epoch 1000/5000, Training Loss: 0.571448028087616, Validation Loss: 0.46390071511268616\n",
            "Epoch 1500/5000, Training Loss: 0.3825329542160034, Validation Loss: 0.468228816986084\n",
            "Epoch 2000/5000, Training Loss: 0.07898759841918945, Validation Loss: 0.4665388762950897\n",
            "Epoch 2500/5000, Training Loss: 0.10419765114784241, Validation Loss: 0.4629688858985901\n",
            "Epoch 3000/5000, Training Loss: 0.08431097865104675, Validation Loss: 0.4802996814250946\n",
            "Epoch 3500/5000, Training Loss: 0.19947503507137299, Validation Loss: 0.5038023591041565\n",
            "Epoch 4000/5000, Training Loss: 0.04506227746605873, Validation Loss: 0.530514121055603\n",
            "Epoch 4500/5000, Training Loss: 0.08443551510572433, Validation Loss: 0.5555590391159058\n",
            "Epoch 5000/5000, Training Loss: 0.07330196350812912, Validation Loss: 0.5717246532440186\n"
          ]
        }
      ]
    }
  ]
}